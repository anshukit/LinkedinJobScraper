import os
import re
import json
import logging
from pathlib import Path
from typing import List, Dict
from urllib.parse import unquote
import time
import requests
import pandas as pd
import streamlit as st
import time   # to use time.sleep
from datetime import datetime, timedelta

from config import config
from utils import DataProcessor
from selenium_urlfix import LinkedInLinkResolver

# =========================
# Config / Logging
# =========================
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("dashboard")


# SCRAPE_API_URL = "http://52.168.130.75:8001/scrape-linkedin"

st.set_page_config(
    page_title="LinkedIn Scraper Dashboard",
    page_icon="ğŸ’¼",
    layout="wide",
    initial_sidebar_state="expanded",
)


API_BASE = "http://52.168.130.75:8001"   # or server IP
SCRAPE_API_URL = f"{API_BASE}/scrape-linkedin"
STATUS_API_URL = f"{API_BASE}/status"
from keywords_config import ALL_KEYWORDS
# =========================
# Helpers - Jobs Viewer
# =========================
def convert_to_view_url(url: str) -> str:
    """Convert LinkedIn search URL (with currentJobId) to canonical /jobs/view/<id>/"""
    if not url or "/jobs/view/" in url:
        return url
    m = re.search(r"currentJobId=(\d+)", url)
    return f"https://www.linkedin.com/jobs/view/{m.group(1)}/" if m else url

def parse_time_ago(time_str: str):
    """Turn 'X hours ago' into datetime"""
    if not time_str:
        return None
    s = time_str.lower()
    now = datetime.now()
    try:
        if "just now" in s or "moment" in s:
            return now
        m = re.search(r"(\d+)\s*minute", s)
        if m:
            return now - datetime.timedelta(minutes=int(m.group(1)))
        h = re.search(r"(\d+)\s*hour", s)
        if h:
            return now - datetime.timedelta(hours=int(h.group(1)))
        d = re.search(r"(\d+)\s*day", s)
        if d:
            return now - datetime.timedelta(days=int(d.group(1)))
        w = re.search(r"(\d+)\s*week", s)
        if w:
            return now - datetime.timedelta(weeks=int(w.group(1)))
    except:
        return None
    return None

def get_time_category(time_str: str) -> str:
    dt = parse_time_ago(time_str)
    if not dt:
        return "older-job"
    age = datetime.now() - dt
    if age < datetime.timedelta(hours=24):
        return "recent-job"
    if age < datetime.timedelta(days=7):
        return "this-week-job"
    return "older-job"

def format_time_display(time_str: str) -> str:
    if not time_str:
        return "Unknown time"
    return time_str.replace("ago", "").strip() + " ago"
@st.cache_data(show_spinner=False)
def load_jobs_data(filtered_jobs_path: Path) -> List[Dict]:
    """Load jobs, dedupe by URL, sort newest first"""
    if not filtered_jobs_path.exists():
        return []
    try:
        raw = filtered_jobs_path.read_text(encoding="utf-8").strip()
        if not raw:  # âœ… handle empty file
            logger.warning(f"âš ï¸ File is empty: {filtered_jobs_path}")
            return []
        jobs = json.loads(raw)  # safe JSON load

        # Deduplicate
        unique = {}
        for j in jobs:
            url = convert_to_view_url(j.get("job_url"))
            if url and url not in unique:
                j["job_url"] = url
                jp = j.get("date_posted", "")
                j["formatted_time"] = format_time_display(jp)
                j["time_category"] = get_time_category(jp)
                unique[url] = j
        return sorted(
            unique.values(),
            key=lambda x: parse_time_ago(x.get("date_posted", "")) or datetime.min,
            reverse=True,
        )
    except Exception as e:
        logger.exception("Error loading jobs")
        st.error(f"Error loading jobs: {e}")
        return []

def display_job_details(job: Dict) -> str:
    details = []
    if job.get("applicants"):
        details.append(f"ğŸ‘¥ {job['applicants']}")
    if job.get("promoted"):
        details.append("â­ Promoted by hirer")
    if job.get("response_info"):
        details.append(f"ğŸ“© {job['response_info']}")
    return " Â· ".join(details) if details else "No additional details"

def display_job_cards(jobs: List[Dict]):
    for job in jobs:
        color = (
            "#2ecc71" if job["time_category"] == "recent-job"
            else "#3498db" if job["time_category"] == "this-week-job"
            else "#95a5a6"
        )
        st.markdown(
            f"""
            <div style="
                border-left: 4px solid {color};
                padding: 1rem;
                margin: 0.8rem 0;
                background: #f8f9fa;
                border-radius: 0.5rem;
                box-shadow: 0 2px 4px rgba(0,0,0,0.08);
            ">
                <div style="font-weight:600; font-size:1.15rem; color:#0e5a8a;">
                    {job.get("title","No title")}
                </div>
                <div style="display:flex; justify-content:space-between; font-size:0.9rem; margin-top:0.2rem;">
                    <span>{job.get("company","Unknown")}</span>
                    <span>â±ï¸ {job.get("formatted_time","Unknown")}</span>
                </div>
                <div style="color:#5f6b7a; margin:0.5rem 0;">ğŸ“ {job.get("location","Unknown")}</div>
                <div style="font-size:0.85rem; color:#738694;">{display_job_details(job)}</div>
                <a href="{job.get("job_url","#")}" target="_blank">
                    <button style="margin-top:0.5rem; background:#0e5a8a; color:white; border:none; padding:0.35rem 1rem; border-radius:0.3rem; cursor:pointer;">
                        ğŸ” View Job
                    </button>
                </a>
            </div>
            """,
            unsafe_allow_html=True,
        )

# =========================
# Apply Links Viewer
# =========================
class DataLoader:
    def __init__(self):
        self.file_paths = config.get_file_paths()  # expects keys like: "apply_posts", "filtered_jobs", etc.
        self.data_processor = DataProcessor()

    def load_apply_links(self) -> List[Dict]:
        """Read JSONL posts and extract any apply links"""
        try:
            data = self.data_processor.file_manager.load_jsonl_safe(self.file_paths["apply_posts"])
            valid = []
            for item in data:
                raw = item.get("Apply_Link") or item.get("link") or item.get("url") or item.get("apply_link")
                if raw and raw != "Not Provided":
                    valid.append(
                        {
                            "original_link": raw,
                            "role": item.get("Role", "Unknown"),
                            "company": item.get("Company", "Unknown"),
                            "eligibility": item.get("Eligibility", "Unknown"),
                            "post_index": item.get("Post_Index", 0),
                        }
                    )
            return valid
        except Exception as e:
            logger.exception("Error loading apply links")
            st.error(f"Error loading apply links: {e}")
            return []

@st.cache_data(show_spinner=False)
def check_link_validity(url: str, timeout: int = 6) -> bool:
    try:
        r = requests.get(url, allow_redirects=True, timeout=timeout)
        return r.status_code in (200, 301, 302, 405)
    except Exception:
        return False

def show_apply_links(data_loader: DataLoader):
    st.header("Apply Links Viewer")

    # Button to load links
    if st.button("ğŸ“¥ Load Apply Links", key="load_apply_links"):
        links_data = data_loader.load_apply_links()

        if not links_data:
            st.warning("No apply links found. Run the main application first to generate data.")
            return

        st.success(f"Found {len(links_data)} apply links")
        resolver = LinkedInLinkResolver()

        progress_bar = st.progress(0, text="Processing links...")
        total = len(links_data)

        for idx, item in enumerate(links_data, start=1):
            with st.expander(f"{item['role']} at {item['company']}", expanded=False):
                st.write(f"**Eligibility:** {item['eligibility']}")

                raw_links = item['original_link']

                # Normalize and deduplicate
                if isinstance(raw_links, str):
                    links = list(set([unquote(link.strip()) for link in raw_links.replace('\n', ',').split(',') if link.strip()]))
                elif isinstance(raw_links, list):
                    links = list(set([unquote(link.strip()) for link in raw_links if isinstance(link, str) and link.strip()]))
                else:
                    links = []

                valid_links = []
                for link in links:
                    if link.startswith("http") and check_link_validity(link):
                        valid_links.append(link)

                if not valid_links:
                    st.warning("âš ï¸ No valid links found for this post.")
                else:
                    st.markdown("**Apply Link(s):**")
                    for link in valid_links:
                        resolved_link = resolver.resolve_url(link)
                        st.markdown(f"- [ğŸ”— {resolved_link}]({resolved_link})", unsafe_allow_html=True)

            # Update progress bar
            progress = idx / total
            progress_bar.progress(progress, text=f"{int(progress * 100)}% processed")

        progress_bar.empty()
        st.success("âœ… All apply links processed.")
    else:
        st.info("Click the **Load Apply Links** button to view data.")


# =========================
# Analytics & System Status
# =========================
def show_analytics(dp: DataProcessor):
    try:
        stats = dp.get_data_statistics()
        st.subheader("ğŸ“Š Overview")
        metrics = stats.get("summary", {})
        c1, c2, c3, c4, c5 = st.columns(5)
        c1.metric("Total Posts", metrics.get("total_posts", 0))
        c2.metric("Eligible Posts", metrics.get("eligible_posts", 0))
        c3.metric("Email Contacts", metrics.get("posts_with_emails", 0))
        c4.metric("Apply Links", metrics.get("posts_with_apply_links", 0))
        c5.metric("Emails Sent", metrics.get("emails_sent", 0))

        st.subheader("ğŸ“ Data Files")
        files = stats.get("files", {})
        table = []
        for k, info in files.items():
            table.append(
                {
                    "File": k.replace("_", " ").title(),
                    "Records": info.get("count", 0),
                    "Size (KB)": round((info.get("file_size", 0) or 0) / 1024, 2),
                    "Status": "âœ… Available" if info.get("file_exists") else "âŒ Missing",
                }
            )
        if table:
            st.dataframe(pd.DataFrame(table), use_container_width=True)
    except Exception as e:
        logger.exception("Analytics failed")
        st.error(f"Error loading analytics: {e}")

def get_dir_size_kb(directory: str) -> float:
    if not os.path.exists(directory):
        return 0.0
    total = 0
    for f in os.listdir(directory):
        p = os.path.join(directory, f)
        if os.path.isfile(p):
            total += os.path.getsize(p)
    return total / 1024

def show_system_status():
    st.subheader("ğŸ’» System Information")
    c1, c2 = st.columns(2)
    with c1:
        st.write("**Configuration Status:**")
        checks = [
            ("Resume File", os.path.exists(config.resume_path)),
            ("LinkedIn Cookie", bool(config.linkedin_cookie)),
            ("Email Accounts", len(config.email_accounts) > 0),
            ("Azure OpenAI", bool(getattr(config.ai, "api_key", None))),
            ("Data Directory", os.path.exists(config.data_dir)),
            ("Logs Directory", os.path.exists(config.logs_dir)),
        ]
        for name, status in checks:
            st.write(("âœ…" if status else "âŒ") + f" {name}")
    with c2:
        st.write("**File System:**")
        try:
            st.write(f"ğŸ“ Data Directory: {get_dir_size_kb(config.data_dir):.1f} KB")
            st.write(f"ğŸ“‹ Logs Directory: {get_dir_size_kb(config.logs_dir):.1f} KB")
        except Exception as e:
            st.write(f"âŒ Error checking directories: {e}")

# =========================
# Main
# =========================
def main():
    st.title("ğŸ’¼ LinkedIn Jobs Dashboard")
    st.caption(f"Last updated: {datetime.now().strftime('%b %d, %Y %I:%M %p')}")

    # Init helpers
    dp = DataProcessor()
    loader = DataLoader()
    file_paths = config.get_file_paths()
    filtered_jobs_path = Path(file_paths.get("filtered_jobs", "data/filtered_jobs.json"))

    # Sidebar: Scraping Controls + Stats
    with st.sidebar:
        # =========================
        # Resume Upload
        # =========================
        st.header("ğŸ“„ Upload Resume")
        uploaded_resume = st.file_uploader("Upload your Resume (PDF)", type=["pdf"])

        if uploaded_resume is not None:
            resume_path = os.path.join("data", "resume.pdf")
            os.makedirs("data", exist_ok=True)
            with open(resume_path, "wb") as f:
                f.write(uploaded_resume.read())
            st.success("âœ… Resume uploaded successfully!")

        # =========================
        # Scraping Controls
        # =========================
        st.header("âš™ï¸ Scraping Controls")

        exp = st.number_input("Years of Experience", min_value=0, max_value=50, value=2, step=1)
        total_posts = st.number_input("Total Posts", min_value=1, max_value=500, value=25, step=1)
        mode = st.selectbox("Scrape Mode", ["jobsData", "postData"])

         
        selected_keywords = st.sidebar.multiselect(
            "Select Keywords",
            options=ALL_KEYWORDS,
            default=["Java Developer", "Python Developer"]
        )
        # ğŸš€ Start Scraping
        if st.button("ğŸš€ Start Scraping", use_container_width=True):
            if not selected_keywords:
                st.error("âŒ Please select at least one keyword")
            else:
                payload = {
                    "experience": exp,
                    "total_posts": int(total_posts),
                    "mode": mode,
                    "keywords": selected_keywords,
                }
                try:
                    res = requests.post(SCRAPE_API_URL, json=payload, timeout=240)
                    if res.status_code == 200:
                        data = res.json()
                        task_id = data.get("task_id")
                        if not task_id:
                            st.error("âŒ Task ID not returned")
                        else:
                            st.session_state["active_task_id"] = task_id
                            st.success(f"âœ… Task started (ID={task_id})")
                    else:
                        st.error(f"âŒ Scrape failed (HTTP {res.status_code})")
                except Exception as e:
                    st.error(f"Error: {e}")

        # ğŸ“Š Progress Tracking
        if "active_task_id" in st.session_state:
            task_id = st.session_state["active_task_id"]

            st.markdown("---")
            st.subheader("ğŸ“Š Progress")
            progress_bar = st.progress(0)
            status_text = st.empty()

            while True:
                try:
                    res = requests.get(f"{STATUS_API_URL}/{task_id}", timeout=240).json()
                    progress = res.get("progress", 0)
                    status = res.get("status", "Starting...")

                    progress_bar.progress(progress)
                    status_text.text(status)

                    if progress >= 100 or "Error" in status:
                        break
                    time.sleep(2)

                except Exception as e:
                    st.error(f"âš ï¸ Error fetching status: {e}")
                    break
            
            if "Error" in status:
                st.error(f"âŒ Failed: {status}")
            else:
                st.success("ğŸ‰ Completed Successfully!")
                if mode == "postData":
                    result = res.get("result", {})
                    if result:
                        st.subheader("ğŸ“§ Email Sending Summary")
                        total_sent = sum(v.get("sent", 0) for v in result.values())
                        total_skipped = sum(v.get("skipped", 0) for v in result.values())
                        st.write(f"âœ… **Total Sent:** {total_sent}")
                        st.write(f"âš ï¸ **Total Skipped:** {total_skipped}")
                    else:
                        st.info("ğŸ“­ No email sending records available.")

        if st.button("ğŸ”„ Refresh Data", use_container_width=True):
            st.cache_data.clear()
            st.rerun()

        # =========================
        # Quick Stats
        # =========================
        st.markdown("---")
        st.subheader("ğŸ“ˆ Quick Stats")
        try:
            stats = dp.get_data_statistics()
            col1, col2 = st.columns(2)
            with col1:
                st.metric("Total Posts", stats["summary"]["total_posts"])
                st.metric("Eligible", stats["summary"]["eligible_posts"])
            with col2:
                st.metric("Emails", stats["summary"]["posts_with_emails"])
                st.metric("Apply Links", stats["summary"]["posts_with_apply_links"])
            if "last_scrape_result" in st.session_state:
                with st.expander("Last Scrape Result"):
                    st.json(st.session_state["last_scrape_result"])
        except Exception as e:
            st.info("Run scraper once to populate stats.")
            logger.debug(e)

    # =========================
    # Tabs
    # =========================
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ’¼ Jobs Data", "ğŸ”— Apply Links", "ğŸ“Š Analytics", "ğŸ§° System Status"])

    with tab1:
        jobs = load_jobs_data(filtered_jobs_path)
        if jobs:
            recent = [j for j in jobs if j["time_category"] == "recent-job"]
            week = [j for j in jobs if j["time_category"] == "this-week-job"]
            older = [j for j in jobs if j["time_category"] == "older-job"]

            if recent:
                st.markdown(f"### ğŸ†• Last 24 Hours ({len(recent)})")
                display_job_cards(recent)
            if week:
                st.markdown(f"### ğŸ—“ This Week ({len(week)})")
                display_job_cards(week)
            if older:
                st.markdown(f"### ğŸ“… Older ({len(older)})")
                display_job_cards(older)
        else:
            st.info("No jobs found. Click **Start Scraping** to generate data.")

    with tab2:
        show_apply_links(loader)

    with tab3:
        show_analytics(dp)

    with tab4:
        show_system_status()


if __name__ == "__main__":
    main()
